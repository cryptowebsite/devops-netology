# Lesson 10.6

## 1. Составьте постмотрем

### 1.1 Краткое описание инцидента
21.10.2018 около 23 часов, на нескольких серверах повредились несколько сетевых разделов. В результате это привело к сбою БД.

### 1.2 Предшествующие события
Плановые работы по замене вышедшего из строя оптического оборудования

### 1.3 Причина инцидента
Из-за потери связи между ЦОД, началось переключение мастрен-ноды. На момент восстановления связи, новая/активная мастер-нода, уже успела получить ряд новых транзакций. В связи с чем случилась проблема согласованности данных.

### 1.4 Воздействие
Сбой БД привел к появлению непоследовательной/устаревшей информации на веб-сайте.

### 1.5 Обнаружение
Инцидент был замечен инженерами группы быстрого реагирования, затем к решению присоединился координатор инцидента.

### 1.6 Реакция
Инцидент был устранен за 24 часа 11 минут.

### 1.7 Восстановление
Восстановили из резервной копии данные на затронутых кластеров MySQL. Сделали их репликации. После синхронизации сделали аварийное переключение на исходную топологию. Сбалансировали возросшую нагрузку. Все ожидающие сборки вебхуков и страниц были обработаны, и была подтверждена целостность и правильная работа всех систем.

### 1.8 Таймлайн
22:52 плановые работы по замене вышедшего из строя оптического оборудования 100G
22:54 системы мониторинга начали генерировать оповещения, указывающие на многочисленные сбои в наших системах
23:02 инженеры группы быстрого реагирования определили, что топологии многочисленных кластеров баз данных находятся в непредвиденном состоянии
23:07 начали вручную блокировать наш внутренний инструмент развертывания
23:09 команда респондентов поместила сайт в желтый статус
23:11 присоединился координатор инцидента
23:13 координатор инцидента и через две минуты изменил статус решения на красный. Были вызваны дополнительные инженеры из группы разработки баз данных GitHub
23:19 остановка выполнение заданий, записывающих метаданные о таких вещах, как push-уведомления
00:05 начали разработку плана по устранению несоответствий данных и внедрению наших процедур аварийного переключения для MySQL
00:41 был инициирован процесс резервного копирования для всех затронутых кластеров MySQL
06:51 несколько кластеров завершили восстановление из резервных копий в центре обработки данных на восточном побережье США и начали репликацию новых данных с западного побережья
07:46 GitHub опубликовал сообщение в блоге об этом инциденте
11:12 все первичные базы данных снова установлены на Восточном побережье США
13:15 начали предоставлять дополнительные реплики чтения MySQL в общедоступном облаке восточного побережья США
16:24 выполнили аварийное переключение на исходную топологию
16:45 сбалансировали возросшую нагрузку
23:03 все ожидающие сборки вебхуков и страниц были обработаны, и была подтверждена целостность и правильная работа всех систем. Статус сайта был обновлен до зеленого

### 1.9 Последующие действия
После анализа текущего инцидента был выявлен ряд технических инициатив
* настроить конфигурацию Orchestrator, чтобы предотвратить продвижение основных баз данных через региональные границы
* начать системную практику проверки сценариев сбоев
